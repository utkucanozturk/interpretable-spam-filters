{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Interpretable Spam Filters \u00b6 This project develops a predictive model to make classification of emails as spam or non-spam. The model was trained on a dataset provided by University of California Irvine Machine Learning Repository . Our pipeline includes exploratory data analysis, hyperparameter tuning, model selection and model interpretation parts. The code in this project is commited by Utku Can Ozturk and Cornelia Gruber .","title":"Home"},{"location":"#interpretable-spam-filters","text":"This project develops a predictive model to make classification of emails as spam or non-spam. The model was trained on a dataset provided by University of California Irvine Machine Learning Repository . Our pipeline includes exploratory data analysis, hyperparameter tuning, model selection and model interpretation parts. The code in this project is commited by Utku Can Ozturk and Cornelia Gruber .","title":"Interpretable Spam Filters"},{"location":"conc/","text":"Conclusion \u00b6 In this analysis, we tried to understand how predictions for email spam probability are made and especially how scammers can improve the contents of their emails, so they pass spam filters. By looking at Feature Importance , we found out which words or characters are most relevant for the different classification algorithms random forest and SVM. Both models agree that charExclamation , i.e. the frequency of the exclamation mark ( ! ), remove and free are essential for prediction. In Relationship between Target & Features , we analyzed the direction in which the features impact the prediction. Most features push the prediction to spam , while personalized words like the recipients name george or the company he works at hp decrease the spam probability. Scammers should avoid creating feelings of urgency by overusing exclamation marks and words in capital letters. In Interactions , we discovered that interactions between features have no crucial impact on the prediction and the choosing the right amount of word/character occurrences can be done independently of other features. By approximating the models with surrogate trees in Global Surrogate Models , we confirmed that personalization, as well as decreasing the number of finance related words, decreases the spam probability. Those findings could also be seen when analyzing specific emails with the help of Shapley values in Local Methods . Finally, feature importance and the relationship between the target and the features based on Shapley values was found to be consistent with prior findings in Comparison of Interpretable Methods . All those findings might help scammers to write emails, which are not obviously spam. Even though this analysis was based on the scammers perspective, understanding spam filters is essential for email providers as well, to not discriminate unfairly. How spam filters work might have a bigger impact on society as one might think. As AlgorithmWatch, a non-profit research organization that evaluates algorithmic decision-making processes, showed an internship application does not pass outlook's spam filter when the word Nigeria is included (see this article ){target=_blank}. Interpretable Machine Learning is a substantial method for understanding hidden bias and for bringing transparency into black box models and more research needs to be done.","title":"Conclusion"},{"location":"conc/#conclusion","text":"In this analysis, we tried to understand how predictions for email spam probability are made and especially how scammers can improve the contents of their emails, so they pass spam filters. By looking at Feature Importance , we found out which words or characters are most relevant for the different classification algorithms random forest and SVM. Both models agree that charExclamation , i.e. the frequency of the exclamation mark ( ! ), remove and free are essential for prediction. In Relationship between Target & Features , we analyzed the direction in which the features impact the prediction. Most features push the prediction to spam , while personalized words like the recipients name george or the company he works at hp decrease the spam probability. Scammers should avoid creating feelings of urgency by overusing exclamation marks and words in capital letters. In Interactions , we discovered that interactions between features have no crucial impact on the prediction and the choosing the right amount of word/character occurrences can be done independently of other features. By approximating the models with surrogate trees in Global Surrogate Models , we confirmed that personalization, as well as decreasing the number of finance related words, decreases the spam probability. Those findings could also be seen when analyzing specific emails with the help of Shapley values in Local Methods . Finally, feature importance and the relationship between the target and the features based on Shapley values was found to be consistent with prior findings in Comparison of Interpretable Methods . All those findings might help scammers to write emails, which are not obviously spam. Even though this analysis was based on the scammers perspective, understanding spam filters is essential for email providers as well, to not discriminate unfairly. How spam filters work might have a bigger impact on society as one might think. As AlgorithmWatch, a non-profit research organization that evaluates algorithmic decision-making processes, showed an internship application does not pass outlook's spam filter when the word Nigeria is included (see this article ){target=_blank}. Interpretable Machine Learning is a substantial method for understanding hidden bias and for bringing transparency into black box models and more research needs to be done.","title":"Conclusion"},{"location":"data/","text":"Dataset \u00b6 The dataset was taken from the UCI Machine Learning Repository . It contains information about the content of emails, i.e., occurrences of words and special characters and information about the frequency and run-length of capital letters, as well as the binary target variable indicating whether an email is spam or non-spam \\cite{Dua:2019}. The dataset includes 4601 emails and 57 features. In detail, there are: 48 continuous attributes that measure the frequency of specific words or numbers in the email as a fraction of total words and range from 0 to 100[%]. E.g. \"free\", \"money\", \"business\" or \"meeting\". 6 continuous attributes that measure the frequency of special characters as a fraction of all characters in the email. Special characters are: semicolon ( ; ), round bracket ( ( ), square bracket ( [ ), exclamation mark ( ! ), dollar sign ( $ ) and hash ( # ). 3 continuous attributes which measure the average ( capitalAve ), longest ( capitalLong ) and total ( capitalTotal ) run-length of capital letters. 1 binary feature indicating the type of mail and is either spam or non-spam . All emails in the dataset came from Hewlett-Packard (HP) employees.While spam emails came from various employees, non-spam emails were collected by George Forman, who works in Palo Alto in California (area code 650), hence the word george and the area code 650 are indicators of non-spam. Since the dataset contains some personalized features, it is possible to assess the effect of email personalization on spam classification. Correlations between variables with highest correlations The dataset does not include any missing feature values and around 40% of the instances have the class label nonspam whereas the rest is spam . Therefore, the dataset can be described as a rather, but not perfectly balanced dataset in terms of target variable distribution. The correlations between features are analyzed and it can be seen that most pairs of variables have an absolute correlation coefficient between 0 and 0.5, indicating a low or negligible correlation. Therefore, only strongly correlated features, i.e., features that have at least one pair where the absolute correlation is 0.5 or higher, are visualized above. There it can be seen that words related to Hewlett Packard like hp , hpl , technology , lab and numbers are among the strongly correlated features. The perfect positive correlation between num857 and num415 might stem from successive digits of a telephone number, which likely belongs to the Hewlett Packard Headquarter and is part of an email signature. Other than that, there are little to no correlations among the frequency of words, numbers, characters or the occurrence of capital letters. In addition, the correlations between the variables and the target variable (indicating spam/non-spam) are analyzed and those that have a higher absolute correlation coefficient than 0.2 are visualized below. Correlations between variables and target variables ranked by coefficient In the graph above, you can see that the variables your , num000 , remove , charDollar , free , business and hp have an absolute correlation with the target variable higher than 0.25. Some correlations, like free and charDollar are not surprising, since a common spam tactic is to promise free money, e.g. \"click this link to get free 100000$!\". Since some words, like your or remove are not inherently suspicious, it is interesting to analyze their impact on the prediction when proper machine learning models are used. Furthermore, the frequency of some interesting words and characters in spam and non-spam emails is demonstrated in the graph below. With this plot, we try to get a better sense of how these highly correlated words are distributed in the emails. Word and Character frequencies in spam and non-spam emails It can be seen that personalized words like hp and george almost exclusively exist in non-spam emails. It is interesting whether this observation will be proven with the more in-depth analysis or whether this is solely a coincidence. Moreover, variables such as charDollar , remove , charExclamation and free exist more frequently in spam emails compared to non-spam emails. Now, that we have a initial understanding of the dataset, we want to first build machine learning models, which can classify emails as spam or non-spam as precise as possible (see section Modelling ) and then analyze their behaviour with interpretable machine learning methods to uncover non-obvious connections (see section Interpretable Machine Learning ).","title":"Dataset"},{"location":"data/#dataset","text":"The dataset was taken from the UCI Machine Learning Repository . It contains information about the content of emails, i.e., occurrences of words and special characters and information about the frequency and run-length of capital letters, as well as the binary target variable indicating whether an email is spam or non-spam \\cite{Dua:2019}. The dataset includes 4601 emails and 57 features. In detail, there are: 48 continuous attributes that measure the frequency of specific words or numbers in the email as a fraction of total words and range from 0 to 100[%]. E.g. \"free\", \"money\", \"business\" or \"meeting\". 6 continuous attributes that measure the frequency of special characters as a fraction of all characters in the email. Special characters are: semicolon ( ; ), round bracket ( ( ), square bracket ( [ ), exclamation mark ( ! ), dollar sign ( $ ) and hash ( # ). 3 continuous attributes which measure the average ( capitalAve ), longest ( capitalLong ) and total ( capitalTotal ) run-length of capital letters. 1 binary feature indicating the type of mail and is either spam or non-spam . All emails in the dataset came from Hewlett-Packard (HP) employees.While spam emails came from various employees, non-spam emails were collected by George Forman, who works in Palo Alto in California (area code 650), hence the word george and the area code 650 are indicators of non-spam. Since the dataset contains some personalized features, it is possible to assess the effect of email personalization on spam classification. Correlations between variables with highest correlations The dataset does not include any missing feature values and around 40% of the instances have the class label nonspam whereas the rest is spam . Therefore, the dataset can be described as a rather, but not perfectly balanced dataset in terms of target variable distribution. The correlations between features are analyzed and it can be seen that most pairs of variables have an absolute correlation coefficient between 0 and 0.5, indicating a low or negligible correlation. Therefore, only strongly correlated features, i.e., features that have at least one pair where the absolute correlation is 0.5 or higher, are visualized above. There it can be seen that words related to Hewlett Packard like hp , hpl , technology , lab and numbers are among the strongly correlated features. The perfect positive correlation between num857 and num415 might stem from successive digits of a telephone number, which likely belongs to the Hewlett Packard Headquarter and is part of an email signature. Other than that, there are little to no correlations among the frequency of words, numbers, characters or the occurrence of capital letters. In addition, the correlations between the variables and the target variable (indicating spam/non-spam) are analyzed and those that have a higher absolute correlation coefficient than 0.2 are visualized below. Correlations between variables and target variables ranked by coefficient In the graph above, you can see that the variables your , num000 , remove , charDollar , free , business and hp have an absolute correlation with the target variable higher than 0.25. Some correlations, like free and charDollar are not surprising, since a common spam tactic is to promise free money, e.g. \"click this link to get free 100000$!\". Since some words, like your or remove are not inherently suspicious, it is interesting to analyze their impact on the prediction when proper machine learning models are used. Furthermore, the frequency of some interesting words and characters in spam and non-spam emails is demonstrated in the graph below. With this plot, we try to get a better sense of how these highly correlated words are distributed in the emails. Word and Character frequencies in spam and non-spam emails It can be seen that personalized words like hp and george almost exclusively exist in non-spam emails. It is interesting whether this observation will be proven with the more in-depth analysis or whether this is solely a coincidence. Moreover, variables such as charDollar , remove , charExclamation and free exist more frequently in spam emails compared to non-spam emails. Now, that we have a initial understanding of the dataset, we want to first build machine learning models, which can classify emails as spam or non-spam as precise as possible (see section Modelling ) and then analyze their behaviour with interpretable machine learning methods to uncover non-obvious connections (see section Interpretable Machine Learning ).","title":"Dataset"},{"location":"iml/","text":"Interpretable Machine Learning \u00b6 As shown in previous sections of this report, black box models like SVM and random forest outperformed interpretable algorithms. Therefore, it is necessary to add a layer of interpretability to our analysis. Interpretable machine learning methods will help us deal with opacity of machine learning models and explain hypotheses about attributes and their connection to the target variable. From the scammers' point of view, our goal is to improve spam emails so that they pass spam filters. Therefore, the following questions or hypotheses are analyzed with the help of interpretable machine learning methods: Which measures about the frequency of words, characters or capital letters in the email should a scammer focus on? Which are the most influential ones? Is there a threshold for words, characters or capital letters that the scammer's email should not exceed? Are there combinations of words which lead to classification as spam? More precisely, Do many exclamation marks lead to a classification as spam? Do many words in capital letters influence the prediction? Do some characters influence the model only within certain thresholds? Are there strong interactions between finance related words or characters, e.g. free , money , credit or $ ? In general, our aim is to uncover interesting connections between the features and the target variable. It is also of interest if those connections differ between different model classes. E.g are the same words important for predictions in kernel-based algorithms (like SVMs) and tree-based models (like random forests)? For this purpose, interpretable machine learning methods such as feature importance, the relationship between target and features (ICE/PDP/ALE), interactions, surrogate models and local models will be introduced and applied to black-box models. Feature Importance \u00b6 Permutation feature importance is, as the name suggests, a method to quantify the importance of a feature. It is measured by calculating the increase in the model\u2019s prediction error after permuting the feature of interest. We measure the prediction error for a specific model with 1-AUC so that a higher AUC leads to a lower error. Since the absolute value is not informative, the ratio between the error of the permuted model and the original model is calculated instead. Intuitively, permuting or shuffling all values of a feature destroys any relationship between the given feature and the target. If the error increases by breaking the relationship, the given feature must have been important for model prediction. Permutation Feature Importance for random forest and SVM Analyzing the figure above enables us to make statements about the permutation feature importance, such as: Permuting the proportion of exclamation marks ( ! ) in an email results in a 10 times increase of 1-AUC compared to the original model, when using the personalized random forest model. For non-personalized random forest, the error increases even fifteenfold. The top-3 important features ( ! , remove and free ) are the same for the personalized and non-personalized random forest model, as well as for the non-personalized SVM model. Considering the scammers do not know which spam filter is used, this consistency is helpful for setting up rules. hp is one of the five most important features in the personalized random forest model, with an importance of 4.4. Knowing the recipient's employer is therefore quite important for discriminating between spam and non-spam. The personalized words george and hp are by far the most important features for personalized SVM with a feature importance of 3.6, or 3.4 respectively. If there is the possibility to determine the recipient's name or other personal information it should be mentioned in the email as well. The feature importance for most features in random forest is notably higher than for SVM. This might be due to the fact that in random forests splits happen based on specific features, whereas SVMs rely on support vectors, which are specific observations and are therefore made up of all features. However statements on how to improve spam mails cannot be made. The importance of features is quite consistent whether personalized features are included or not. Therefore, from now on, we will only apply interpretable machine learning methods on the whole dataset, i.e. the dataset including personalized variables. Relationship between Target & Features \u00b6 Now that we know which features are important for the model's prediction, it is now of interest to understand the nature of the impact. Is there a positive or negative influence on the prediction? Is the relationship linear, monotonic, or more complex? One method for answering these questions is to use partial dependency plots (PDP). They can be used to visualize the relationship between one or two features and the target. The partial dependence function is estimated by: \\hat{f}_{x_S}(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C}) where x_S are the features we are interested in and all other features are in x_C . We basically average the prediction for a specific value of x_S over all instances and obtain marginalized effects by this way. The PDP assumes uncorrelated features, since there would be highly unlikely or even impossible feature combinations considered otherwise. Luckily features which were shown to be important in Feature Importance , show little to no correlations and we can apply this method for further insights. If you don't plot the average effect over all instances, but instead draw a line for each observation we obtain the individual conditional expectation (ICE). ICE plots show how the instance\u2019s prediction changes when the feature value changes. One advantage of ICE plots compared to PDP plots is that interactions can be uncovered, if the ICE-lines are not parallel, but cross each other. In figure below you can see that as the number of ! in an email increases, the marginal spam probability increases for both random forest and SVM. For the random forest model, there are several small jumps in the PDP curve (orange line, e.g., at 0.1 or 0.8), while for SVM it is quite smooth. This could be due to the fact that random forest models are based on certain split points for variables, while SVM is based on the distance of observations to support vectors, which is continuous. This relationship is plausible to the extent that many exclamation marks suggest a sense of urgency, which is a common tactic in spam emails. Centered ICE curves (black) and PDP (orange) for \"!\" On the other hand, the presence of the word hp lowers the probability of spam in both the random forest and the SVM (see figure below). This is also plausible, as the knowledge of the recipient's employer shows that the email was sent from a known individual and is not spam sent to multiple people at once. Another explanation might be that the email is sent from within the company and hp is in the sender's email signature. In figure below you can see that for random forest there is a steep drop between 0 and 0.5. Above that, the PDP is almost flat. This indicates that the mere mention of hp is enough to reduce the spam probability, and flooding the email with the employer's name does not help to pass spam filters. Centered ICE curves (black) and PDP (orange) for \"hp\" As the ICE curves for random forest are all parallel, we do not expect much interaction for the variables charExclamation or hp . However, if we closely look at the ICE curve for charExclamation in the SVM model we can see two different courses: for some ICE lines the effect continuously increases for increasing values, whereas some lines seem almost flat for increasing values of charExclamation . This might indicate interactions, which will further be analyzed in Interactions . As already mentioned, partial dependence plots might be misleading, if correlations among features exist. An unbiased and less computationally intensive alternative to partial dependence plots are accumulated local effect (ALE) plots. The accumulated local effects measure the difference in prediction in a small range around the desired feature value, instead of averaging the prediction over all observations. In the figures below you can see the ALE plots for the top 12 most important features based on permutation feature importance (see Section Feature Importance ). In the rugs below each ALE plot it can be seen that most variables are highly right-skewed, i.e. high density in low values and little observations/low density in high feature values. When looking at charExclamation for example, you can see that most emails have values below 10, but there are two outliers, one close to 20 and one above 30. This makes it difficult to properly see the curve of ALE plots in the relevant areas. Thus, PDP plots above were cut off at the 0.95 quantile. Accumulated Local Effects (ALE) for random forest Accumulated Local Effects (ALE) for SVM However, it is possible to see the general direction in which a feature influences the prediction. For random forest, most features have a positive influence on the spam probability. For the features charExclamation , remove , free , etc. we see that the average prediction is below zero at feature values of zero and quickly increases as the feature values increase. This shows that when those words are common in an email the spam probability increases. The only word that lowers spam probability is george . When we now look at the ALE plots for SVM, we make slightly different observations. The three most important features/words rather shift the prediction for an email towards non-spam, whereas in random forest the top four words indicate spam. The words/characters that suggest spam in SVM, like charExclamation , remove , charDollar and free , initially increase the average predicted spam probability. The ALE sharply drops into negative areas at higher values. However, this should not be too relevant for the actual predictions, since almost no emails contain such a high amount of these words/characters. We can therefore conclude that increasing the word or character count for charExclamation , remove , charDollar and free by a reasonable amount increases the spam probability and scammers need to be careful when using those words/characters in their spam mail. Interactions \u00b6 To analyze whether the effect of one feature depends on the value of another feature, we will consider feature interactions. A measurement called H-statistic, which was introduced in this article , can be used as an estimate for interaction strength. The H-statistic measures the share of the prediction function's variance that is explained by the interaction and can be used to assess various kinds of interactions. H-statistics could be very helpful to answer questions like Are there combinations of words which lead to classification as spam? . The H-statistic values for the overall interaction strength of a given feature to all other features can be found in f,gure below. It can be seen that the interactions in Random Forest are rather weak, where the strongest interacting word remove has a interaction strength of 0.15. This means that less than 15% of the variance of \\hat{f} are explained by interactions. H-Statistic Looking at the SVM, there are two features with particularly high interactions, namely receive and charExclamation . The two-way interactions for these features can be seen in figure below. There we also see that almost 100% of the total variance of PD_{jk} is explained by the interaction between receive and email . H-Statistic 2-way Interaction Now, if we further analyze the relationship between the two variables, we see that there is a very strong positive influence on spam probability when the occurrence of email is low and the percentage of receive of all words in the email is above 2% (see figure below). However, there are only two emails with such a high occurrence of the word receive , and they both happen to be spam . Using interpretability methods, we found regions where the SVM strongly overfits, and if this model would be used in real spam filters, we would need to correct this behavior. When looking at the ALE plot in figure below, we can also verify this overfitting, since the curve is almost flat in the range between 0 and 0.5 and sharply drops afterwards. PDP for \"email\" and \"receive\", as well as ALE for \"receive\" Since most H-statistics are quite low, we can say that scammers do not need to take care of interacting words and can try to minimize the spam probability for each feature independently. Global Surrogate Models \u00b6 The general goal of using interpretable machine learning methods is to understand how predictions are formed. Therefore, perhaps the most straightforward method would be to simply estimate the model's predictions by using an inherently interpretable model. This is exactly what global surrogate models do. More specifically, they are simpler interpretable models that are trained to estimate the predictions of the non-interpretable model. By this approach, predictions from random forest and SVM are approximated by a single classification tree with a depth of 2. The surrogate tree for random forest can be seen in the figure below. In the first node, we subset the data based on whether your occurs less or more than in 0.4% of all words. If your makes up at most 0.4% of the words we check the value for charDollar . If charDollar is lower than or equal to 0.055%, the instance will be classified as non-spam, otherwise the email is classified as spam. On the other side of the tree, when the value for your is above 0.4%, the tree splits the data based on the value of hp . If hp is lower than or equal to 0.11%, the email is classified as spam, when it is higher it will be classified as non-spam. The random forest surrogate tree predicts 82.2% of the email classes correctly and has a false positive rate of 18.6%. This suggests that the majority of predictions can be reasonably approximated by the surrogate and scammers should be aware of their use of your , ! and hp . Random Forest Surrogate Tree The surrogate tree for SVM slightly differs from the one for random forest (see the figure below). Just as for random forest the surrogate tree for SVM initially splits at your . However, the threshold is a little higher at 0.59%. The first node where the SVM surrogate differs from the random forest surrogate is when the value for your is below 0.59%. There we split based on the value of num000 and not based on the fraction of $ in the email. If num000 is at most 0.15%, we predict an email to be non-spam, whereas the prediction is spam when 000 make up more than 0.15% of all characters. Interestingly enough, the split-rule when your is above 0.59% is identical to the one from random forest and emails are classified as spam if hp is at most 0.11% and are classified as non-spam otherwise. The SVM surrogate tree has a very similar prediction accuracy of 82.1% as the random forest surrogate tree. However, the SVM surrogate's false positive ratio is a little better with 15%. So for both models it is reasonable to use surrogate models to understand how predictions are made. Concluding we can say that even though both trees share the first split variable the threshold is a little higher in the SVM surrogate. However, if the occurrence of your is above the respective thresholds both trees split equally in the next node. The decision criterion if the email does not exceed the threshold of your differs. The random forest surrogate relies on the variable charDollar , whereas the SVM surrogate splits based on num000 . However, both variables relate to financial scams or money in the broader sens. Since both surrogate trees are quite similar, scammers should be careful when using words related to financial scams and should definitely include the companies name if they can somehow guess it from the email domain. Local Methods \u00b6 Local methods are used to interpret the predictions of the black box model at the instance level. In our case, the goal of using local methods is to understand why the SVM or random forest models made incorrect predictions for some emails. About 40 instances are incorrectly predicted by both models. For better understanding, we will focus on two different instances: One instance that is incorrectly predicted by random forest but correctly predicted by SVM. One instance that is incorrectly predicted by SVM but correctly predicted by random forest. To understand how the models made predictions for those specific emails Shapley values and Ceteris Paribus Profiles will be introduced. Shapley values help us to measure each feature\u2019s contribution to the prediction of an instance, more precisely they measure how much a feature contributed to the prediction compared to the average prediction. They are calculated by averaging the marginal payout or contribution for each feature. Assuming we are interested in the contribution of feature x. First, the prediction for every combination of features not including x is calculated. Then we measure the marginal contribution of x, i.e. how adding x to each combination changes the prediction. Finally, all marginal contributions are averaged and we obtain the Shapley value for feature x. Since there are many possible coalitions of the feature values, calculating Shapley values is computationally expensive. Spam is coded as 1 and non-spam as 0 , each email which has a higher score/prediction than 0.5 will be classified as spam. We will now focus on a spam mail, which was incorrectly predicted as non-spam by random forest with a predicted value of 0.42, but correctly predicted by SVM with a prediction of 0.53 If we look at the figure below, we can see how each feature value changes the prediction from the intercept (vertical dashed line, at 0.385) either in the spam (green/positive values) or non-spam (red/negative values) direction. 0.6\\% of words are remove , george is not mentioned at all and free makes up 0.34% of all words, all this makes the prediction move in the direction of spam . However, hp and hpl (which stands for Hewlett Packard Labs) make up 0.34% of words each and move the prediction in the direction of non-spam . As we saw in Relationship between Target & Features and Global Surrogate Models naming the company is a strong indicator for non-spam and that is why the email was incorrectly predicted by random forest. Shapley values for a chosen spam mail which is incorrectly classified by random forest with a prediction of 0.42. The vertical dashed, blue line shows the average prediction (0.385). The green and red bars show the Shapley values for each feature value. Summing up the intercept/average prediction and all Shapley values results in the predicted value If we now compare which features have high absolute values of Shapley values in the SVM model in figure below, we see that only the average and total run length of capital letters ( capitalAve and capitalTotal ) and the number of ! ( charExclamation ) make the email seem non-spam. There are however many other words that can compensate this, like remove , you or our . Additionally, the longest run length of capital letters is 15 ( capitalLong ) which is quite long and therefore suspicious. Finally, SVM is able to correctly recognize this email as spam. Shapley values for a chosen spam mail which is correctly classified by SVM with a prediction of 0.53 As we can see this email was not obviously spam, as both models predicted values close to the decision threshold of 0.5. It is however important to note that scammers usually do not know which spam filter is applied by certain email providers and there is no way of knowing if the email actually reaches the potential victim. All scammers can do is to avoid obvious spam criteria and hope spam filters can be fooled. Ceteris Paribus Profiles are identical to individual conditional expectation curves (ICE) that were introduced in Relationship between Target & Features . In summary, Ceteris Paribus profiles help us to assess how the model\u2019s prediction would change if the value of a feature changes. In the figure below, it can be seen that even a small increase in the occurrence of remove , as well as the number of exclamation marks or capital letters in the email would increase the predicted value for SVM (blue line). In the random forest model the increase in prediction would not be that substantial (light green line). Increasing the number hp occurs in the email would decrease the prediction for both SVM and random forest. Again, there would be a great decrease for SVM and a smaller decrease for random forest. Ceteris Paribus Profiles for a chosen spam mail We will now consider a non-spam email, which is correctly classified by random forest with a prediction of 0.22 and incorrectly classified by SVM with a prediction of 0.52. In the figure below representing shapley values for random forest prediction, you can see that the occurrence of the word edu has a significant impact on the prediction as non-spam. The positive effect of remove is quite big, but cannot compensate for the negative effect of edu . Random forest is able to correctly classify the email as non-spam. Shapley values for a chosen non-spam mail which is correctly classified by random forest with a prediction of 0.22 Different than in random forest, edu is not one of the top Shapley values for SVM for this email (see figure below). Instead, the SVM focuses more on the fact that words like will , hp , hpl do not exist in the email. Shapley values for a chosen non-spam mail which is incorrectly classified by SVM with a prediction of 0.52 For further analysis, ceteris-paribus profiles for this instance are plotted below. Ceteris Paribus Profiles for a chosen non-spam mail In the plot above, it can be seen that the effect of edu and cs drops more sharply in the random forest model. In this specific email, the SVM prediction changes to non-spam even for a small increase in hp and will . Comparison of Interpretable Methods \u00b6 So far we saw that the different interpretable machine learning methods were consistent with each other concerning how certain feature values impact the prediction. PDP, or ALE plots as well as surrogate models or individual Shapley values showed that personalizing the email is an indicator for non-spam. Whereas many occurrences of exclamation marks or words like free or remove are strong indicators for spam. Now we want to asses whether the feature importance is also consistent when using a Shapley based measure instead of permutation feature importance. For each feature, the Shapley feature importance is the mean absolute value of the Shapley values of all observations. This means that features which have high (absolute) Shapley values for many emails have a high importance in general. A comparison of both importance measures can be found in the figure below. Comparison of Feature Importance Plots for Random Forest For both measures, the frequency of exclamation points is the most important variable for classification as spam or non-spam. In addition, the variables hp , remove , charDollar , free , and variables concerning capital letters have high values in both importance measures with only a slight change in order. Therefore, both importance measures agree with each other and we can reasonably assume that those features are essential for the random forest's prediction. If we now want to assess the shape of the influence on the prediction we can not only look at PDP plots, but on SHAP dependence plots as well. SHAP dependence plots show the marginal effect of a feature value on the prediction, by plotting Shapley values for all emails against the respective feature value. We can therefore see for which feature values the Shapley values are positive and push the prediction to spam and where the Shapley values are negative and decrease the spam probability. The marginal effect of the value of hp on the prediction is visualized with SHAP dependence plot and partial dependence plot in the figure below. Comparison of Feature Importance Plots for Random Forest By looking at the shapley values figure, we see that for hp = 0 most Shapley values are between 0 and 0.05, which especially means they are positive and increase the spam probability. As soon as the value for hp increases we see a sharp drop and almost all Shapley values are between -0.05 and -0.20. Similar to the findings of the PDP plot, increasing the number of hp does not decrease the Shapley value in general. Many Shapley values for hp between 2 and 20 are around -0.05, which is the same value that is already reached at hp = 0.1. Again, we can confirm our findings, that mentioning the company name is enough to decrease the spam probability.","title":"Interpretable Machine Learning"},{"location":"iml/#interpretable-machine-learning","text":"As shown in previous sections of this report, black box models like SVM and random forest outperformed interpretable algorithms. Therefore, it is necessary to add a layer of interpretability to our analysis. Interpretable machine learning methods will help us deal with opacity of machine learning models and explain hypotheses about attributes and their connection to the target variable. From the scammers' point of view, our goal is to improve spam emails so that they pass spam filters. Therefore, the following questions or hypotheses are analyzed with the help of interpretable machine learning methods: Which measures about the frequency of words, characters or capital letters in the email should a scammer focus on? Which are the most influential ones? Is there a threshold for words, characters or capital letters that the scammer's email should not exceed? Are there combinations of words which lead to classification as spam? More precisely, Do many exclamation marks lead to a classification as spam? Do many words in capital letters influence the prediction? Do some characters influence the model only within certain thresholds? Are there strong interactions between finance related words or characters, e.g. free , money , credit or $ ? In general, our aim is to uncover interesting connections between the features and the target variable. It is also of interest if those connections differ between different model classes. E.g are the same words important for predictions in kernel-based algorithms (like SVMs) and tree-based models (like random forests)? For this purpose, interpretable machine learning methods such as feature importance, the relationship between target and features (ICE/PDP/ALE), interactions, surrogate models and local models will be introduced and applied to black-box models.","title":"Interpretable Machine Learning"},{"location":"iml/#feature-importance","text":"Permutation feature importance is, as the name suggests, a method to quantify the importance of a feature. It is measured by calculating the increase in the model\u2019s prediction error after permuting the feature of interest. We measure the prediction error for a specific model with 1-AUC so that a higher AUC leads to a lower error. Since the absolute value is not informative, the ratio between the error of the permuted model and the original model is calculated instead. Intuitively, permuting or shuffling all values of a feature destroys any relationship between the given feature and the target. If the error increases by breaking the relationship, the given feature must have been important for model prediction. Permutation Feature Importance for random forest and SVM Analyzing the figure above enables us to make statements about the permutation feature importance, such as: Permuting the proportion of exclamation marks ( ! ) in an email results in a 10 times increase of 1-AUC compared to the original model, when using the personalized random forest model. For non-personalized random forest, the error increases even fifteenfold. The top-3 important features ( ! , remove and free ) are the same for the personalized and non-personalized random forest model, as well as for the non-personalized SVM model. Considering the scammers do not know which spam filter is used, this consistency is helpful for setting up rules. hp is one of the five most important features in the personalized random forest model, with an importance of 4.4. Knowing the recipient's employer is therefore quite important for discriminating between spam and non-spam. The personalized words george and hp are by far the most important features for personalized SVM with a feature importance of 3.6, or 3.4 respectively. If there is the possibility to determine the recipient's name or other personal information it should be mentioned in the email as well. The feature importance for most features in random forest is notably higher than for SVM. This might be due to the fact that in random forests splits happen based on specific features, whereas SVMs rely on support vectors, which are specific observations and are therefore made up of all features. However statements on how to improve spam mails cannot be made. The importance of features is quite consistent whether personalized features are included or not. Therefore, from now on, we will only apply interpretable machine learning methods on the whole dataset, i.e. the dataset including personalized variables.","title":"Feature Importance"},{"location":"iml/#relationship-between-target-features","text":"Now that we know which features are important for the model's prediction, it is now of interest to understand the nature of the impact. Is there a positive or negative influence on the prediction? Is the relationship linear, monotonic, or more complex? One method for answering these questions is to use partial dependency plots (PDP). They can be used to visualize the relationship between one or two features and the target. The partial dependence function is estimated by: \\hat{f}_{x_S}(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C}) where x_S are the features we are interested in and all other features are in x_C . We basically average the prediction for a specific value of x_S over all instances and obtain marginalized effects by this way. The PDP assumes uncorrelated features, since there would be highly unlikely or even impossible feature combinations considered otherwise. Luckily features which were shown to be important in Feature Importance , show little to no correlations and we can apply this method for further insights. If you don't plot the average effect over all instances, but instead draw a line for each observation we obtain the individual conditional expectation (ICE). ICE plots show how the instance\u2019s prediction changes when the feature value changes. One advantage of ICE plots compared to PDP plots is that interactions can be uncovered, if the ICE-lines are not parallel, but cross each other. In figure below you can see that as the number of ! in an email increases, the marginal spam probability increases for both random forest and SVM. For the random forest model, there are several small jumps in the PDP curve (orange line, e.g., at 0.1 or 0.8), while for SVM it is quite smooth. This could be due to the fact that random forest models are based on certain split points for variables, while SVM is based on the distance of observations to support vectors, which is continuous. This relationship is plausible to the extent that many exclamation marks suggest a sense of urgency, which is a common tactic in spam emails. Centered ICE curves (black) and PDP (orange) for \"!\" On the other hand, the presence of the word hp lowers the probability of spam in both the random forest and the SVM (see figure below). This is also plausible, as the knowledge of the recipient's employer shows that the email was sent from a known individual and is not spam sent to multiple people at once. Another explanation might be that the email is sent from within the company and hp is in the sender's email signature. In figure below you can see that for random forest there is a steep drop between 0 and 0.5. Above that, the PDP is almost flat. This indicates that the mere mention of hp is enough to reduce the spam probability, and flooding the email with the employer's name does not help to pass spam filters. Centered ICE curves (black) and PDP (orange) for \"hp\" As the ICE curves for random forest are all parallel, we do not expect much interaction for the variables charExclamation or hp . However, if we closely look at the ICE curve for charExclamation in the SVM model we can see two different courses: for some ICE lines the effect continuously increases for increasing values, whereas some lines seem almost flat for increasing values of charExclamation . This might indicate interactions, which will further be analyzed in Interactions . As already mentioned, partial dependence plots might be misleading, if correlations among features exist. An unbiased and less computationally intensive alternative to partial dependence plots are accumulated local effect (ALE) plots. The accumulated local effects measure the difference in prediction in a small range around the desired feature value, instead of averaging the prediction over all observations. In the figures below you can see the ALE plots for the top 12 most important features based on permutation feature importance (see Section Feature Importance ). In the rugs below each ALE plot it can be seen that most variables are highly right-skewed, i.e. high density in low values and little observations/low density in high feature values. When looking at charExclamation for example, you can see that most emails have values below 10, but there are two outliers, one close to 20 and one above 30. This makes it difficult to properly see the curve of ALE plots in the relevant areas. Thus, PDP plots above were cut off at the 0.95 quantile. Accumulated Local Effects (ALE) for random forest Accumulated Local Effects (ALE) for SVM However, it is possible to see the general direction in which a feature influences the prediction. For random forest, most features have a positive influence on the spam probability. For the features charExclamation , remove , free , etc. we see that the average prediction is below zero at feature values of zero and quickly increases as the feature values increase. This shows that when those words are common in an email the spam probability increases. The only word that lowers spam probability is george . When we now look at the ALE plots for SVM, we make slightly different observations. The three most important features/words rather shift the prediction for an email towards non-spam, whereas in random forest the top four words indicate spam. The words/characters that suggest spam in SVM, like charExclamation , remove , charDollar and free , initially increase the average predicted spam probability. The ALE sharply drops into negative areas at higher values. However, this should not be too relevant for the actual predictions, since almost no emails contain such a high amount of these words/characters. We can therefore conclude that increasing the word or character count for charExclamation , remove , charDollar and free by a reasonable amount increases the spam probability and scammers need to be careful when using those words/characters in their spam mail.","title":"Relationship between Target &amp; Features"},{"location":"iml/#interactions","text":"To analyze whether the effect of one feature depends on the value of another feature, we will consider feature interactions. A measurement called H-statistic, which was introduced in this article , can be used as an estimate for interaction strength. The H-statistic measures the share of the prediction function's variance that is explained by the interaction and can be used to assess various kinds of interactions. H-statistics could be very helpful to answer questions like Are there combinations of words which lead to classification as spam? . The H-statistic values for the overall interaction strength of a given feature to all other features can be found in f,gure below. It can be seen that the interactions in Random Forest are rather weak, where the strongest interacting word remove has a interaction strength of 0.15. This means that less than 15% of the variance of \\hat{f} are explained by interactions. H-Statistic Looking at the SVM, there are two features with particularly high interactions, namely receive and charExclamation . The two-way interactions for these features can be seen in figure below. There we also see that almost 100% of the total variance of PD_{jk} is explained by the interaction between receive and email . H-Statistic 2-way Interaction Now, if we further analyze the relationship between the two variables, we see that there is a very strong positive influence on spam probability when the occurrence of email is low and the percentage of receive of all words in the email is above 2% (see figure below). However, there are only two emails with such a high occurrence of the word receive , and they both happen to be spam . Using interpretability methods, we found regions where the SVM strongly overfits, and if this model would be used in real spam filters, we would need to correct this behavior. When looking at the ALE plot in figure below, we can also verify this overfitting, since the curve is almost flat in the range between 0 and 0.5 and sharply drops afterwards. PDP for \"email\" and \"receive\", as well as ALE for \"receive\" Since most H-statistics are quite low, we can say that scammers do not need to take care of interacting words and can try to minimize the spam probability for each feature independently.","title":"Interactions"},{"location":"iml/#global-surrogate-models","text":"The general goal of using interpretable machine learning methods is to understand how predictions are formed. Therefore, perhaps the most straightforward method would be to simply estimate the model's predictions by using an inherently interpretable model. This is exactly what global surrogate models do. More specifically, they are simpler interpretable models that are trained to estimate the predictions of the non-interpretable model. By this approach, predictions from random forest and SVM are approximated by a single classification tree with a depth of 2. The surrogate tree for random forest can be seen in the figure below. In the first node, we subset the data based on whether your occurs less or more than in 0.4% of all words. If your makes up at most 0.4% of the words we check the value for charDollar . If charDollar is lower than or equal to 0.055%, the instance will be classified as non-spam, otherwise the email is classified as spam. On the other side of the tree, when the value for your is above 0.4%, the tree splits the data based on the value of hp . If hp is lower than or equal to 0.11%, the email is classified as spam, when it is higher it will be classified as non-spam. The random forest surrogate tree predicts 82.2% of the email classes correctly and has a false positive rate of 18.6%. This suggests that the majority of predictions can be reasonably approximated by the surrogate and scammers should be aware of their use of your , ! and hp . Random Forest Surrogate Tree The surrogate tree for SVM slightly differs from the one for random forest (see the figure below). Just as for random forest the surrogate tree for SVM initially splits at your . However, the threshold is a little higher at 0.59%. The first node where the SVM surrogate differs from the random forest surrogate is when the value for your is below 0.59%. There we split based on the value of num000 and not based on the fraction of $ in the email. If num000 is at most 0.15%, we predict an email to be non-spam, whereas the prediction is spam when 000 make up more than 0.15% of all characters. Interestingly enough, the split-rule when your is above 0.59% is identical to the one from random forest and emails are classified as spam if hp is at most 0.11% and are classified as non-spam otherwise. The SVM surrogate tree has a very similar prediction accuracy of 82.1% as the random forest surrogate tree. However, the SVM surrogate's false positive ratio is a little better with 15%. So for both models it is reasonable to use surrogate models to understand how predictions are made. Concluding we can say that even though both trees share the first split variable the threshold is a little higher in the SVM surrogate. However, if the occurrence of your is above the respective thresholds both trees split equally in the next node. The decision criterion if the email does not exceed the threshold of your differs. The random forest surrogate relies on the variable charDollar , whereas the SVM surrogate splits based on num000 . However, both variables relate to financial scams or money in the broader sens. Since both surrogate trees are quite similar, scammers should be careful when using words related to financial scams and should definitely include the companies name if they can somehow guess it from the email domain.","title":"Global Surrogate Models"},{"location":"iml/#local-methods","text":"Local methods are used to interpret the predictions of the black box model at the instance level. In our case, the goal of using local methods is to understand why the SVM or random forest models made incorrect predictions for some emails. About 40 instances are incorrectly predicted by both models. For better understanding, we will focus on two different instances: One instance that is incorrectly predicted by random forest but correctly predicted by SVM. One instance that is incorrectly predicted by SVM but correctly predicted by random forest. To understand how the models made predictions for those specific emails Shapley values and Ceteris Paribus Profiles will be introduced. Shapley values help us to measure each feature\u2019s contribution to the prediction of an instance, more precisely they measure how much a feature contributed to the prediction compared to the average prediction. They are calculated by averaging the marginal payout or contribution for each feature. Assuming we are interested in the contribution of feature x. First, the prediction for every combination of features not including x is calculated. Then we measure the marginal contribution of x, i.e. how adding x to each combination changes the prediction. Finally, all marginal contributions are averaged and we obtain the Shapley value for feature x. Since there are many possible coalitions of the feature values, calculating Shapley values is computationally expensive. Spam is coded as 1 and non-spam as 0 , each email which has a higher score/prediction than 0.5 will be classified as spam. We will now focus on a spam mail, which was incorrectly predicted as non-spam by random forest with a predicted value of 0.42, but correctly predicted by SVM with a prediction of 0.53 If we look at the figure below, we can see how each feature value changes the prediction from the intercept (vertical dashed line, at 0.385) either in the spam (green/positive values) or non-spam (red/negative values) direction. 0.6\\% of words are remove , george is not mentioned at all and free makes up 0.34% of all words, all this makes the prediction move in the direction of spam . However, hp and hpl (which stands for Hewlett Packard Labs) make up 0.34% of words each and move the prediction in the direction of non-spam . As we saw in Relationship between Target & Features and Global Surrogate Models naming the company is a strong indicator for non-spam and that is why the email was incorrectly predicted by random forest. Shapley values for a chosen spam mail which is incorrectly classified by random forest with a prediction of 0.42. The vertical dashed, blue line shows the average prediction (0.385). The green and red bars show the Shapley values for each feature value. Summing up the intercept/average prediction and all Shapley values results in the predicted value If we now compare which features have high absolute values of Shapley values in the SVM model in figure below, we see that only the average and total run length of capital letters ( capitalAve and capitalTotal ) and the number of ! ( charExclamation ) make the email seem non-spam. There are however many other words that can compensate this, like remove , you or our . Additionally, the longest run length of capital letters is 15 ( capitalLong ) which is quite long and therefore suspicious. Finally, SVM is able to correctly recognize this email as spam. Shapley values for a chosen spam mail which is correctly classified by SVM with a prediction of 0.53 As we can see this email was not obviously spam, as both models predicted values close to the decision threshold of 0.5. It is however important to note that scammers usually do not know which spam filter is applied by certain email providers and there is no way of knowing if the email actually reaches the potential victim. All scammers can do is to avoid obvious spam criteria and hope spam filters can be fooled. Ceteris Paribus Profiles are identical to individual conditional expectation curves (ICE) that were introduced in Relationship between Target & Features . In summary, Ceteris Paribus profiles help us to assess how the model\u2019s prediction would change if the value of a feature changes. In the figure below, it can be seen that even a small increase in the occurrence of remove , as well as the number of exclamation marks or capital letters in the email would increase the predicted value for SVM (blue line). In the random forest model the increase in prediction would not be that substantial (light green line). Increasing the number hp occurs in the email would decrease the prediction for both SVM and random forest. Again, there would be a great decrease for SVM and a smaller decrease for random forest. Ceteris Paribus Profiles for a chosen spam mail We will now consider a non-spam email, which is correctly classified by random forest with a prediction of 0.22 and incorrectly classified by SVM with a prediction of 0.52. In the figure below representing shapley values for random forest prediction, you can see that the occurrence of the word edu has a significant impact on the prediction as non-spam. The positive effect of remove is quite big, but cannot compensate for the negative effect of edu . Random forest is able to correctly classify the email as non-spam. Shapley values for a chosen non-spam mail which is correctly classified by random forest with a prediction of 0.22 Different than in random forest, edu is not one of the top Shapley values for SVM for this email (see figure below). Instead, the SVM focuses more on the fact that words like will , hp , hpl do not exist in the email. Shapley values for a chosen non-spam mail which is incorrectly classified by SVM with a prediction of 0.52 For further analysis, ceteris-paribus profiles for this instance are plotted below. Ceteris Paribus Profiles for a chosen non-spam mail In the plot above, it can be seen that the effect of edu and cs drops more sharply in the random forest model. In this specific email, the SVM prediction changes to non-spam even for a small increase in hp and will .","title":"Local Methods"},{"location":"iml/#comparison-of-interpretable-methods","text":"So far we saw that the different interpretable machine learning methods were consistent with each other concerning how certain feature values impact the prediction. PDP, or ALE plots as well as surrogate models or individual Shapley values showed that personalizing the email is an indicator for non-spam. Whereas many occurrences of exclamation marks or words like free or remove are strong indicators for spam. Now we want to asses whether the feature importance is also consistent when using a Shapley based measure instead of permutation feature importance. For each feature, the Shapley feature importance is the mean absolute value of the Shapley values of all observations. This means that features which have high (absolute) Shapley values for many emails have a high importance in general. A comparison of both importance measures can be found in the figure below. Comparison of Feature Importance Plots for Random Forest For both measures, the frequency of exclamation points is the most important variable for classification as spam or non-spam. In addition, the variables hp , remove , charDollar , free , and variables concerning capital letters have high values in both importance measures with only a slight change in order. Therefore, both importance measures agree with each other and we can reasonably assume that those features are essential for the random forest's prediction. If we now want to assess the shape of the influence on the prediction we can not only look at PDP plots, but on SHAP dependence plots as well. SHAP dependence plots show the marginal effect of a feature value on the prediction, by plotting Shapley values for all emails against the respective feature value. We can therefore see for which feature values the Shapley values are positive and push the prediction to spam and where the Shapley values are negative and decrease the spam probability. The marginal effect of the value of hp on the prediction is visualized with SHAP dependence plot and partial dependence plot in the figure below. Comparison of Feature Importance Plots for Random Forest By looking at the shapley values figure, we see that for hp = 0 most Shapley values are between 0 and 0.05, which especially means they are positive and increase the spam probability. As soon as the value for hp increases we see a sharp drop and almost all Shapley values are between -0.05 and -0.20. Similar to the findings of the PDP plot, increasing the number of hp does not decrease the Shapley value in general. Many Shapley values for hp between 2 and 20 are around -0.05, which is the same value that is already reached at hp = 0.1. Again, we can confirm our findings, that mentioning the company name is enough to decrease the spam probability.","title":"Comparison of Interpretable Methods"},{"location":"intro/","text":"Introduction \u00b6 Emails are an essential part of everyday life. They are used for communication between people both in private life and for business purposes. Generally, emails can be categorized as spam or non-spam, where non-spam emails contain useful or desired information for the recipient. Spam emails, on the other hand, are unwanted by the recipient and can be sent with the purpose of advertising, fraud, etc. According to the Trustwave Global Security Report 2020 , 45% of all emails sent in 2018 were spam. Nowadays, email service providers use spam filters based on various machine learning methods to distinguish between spam and non-spam emails. In the following report, we will take the rather unusual perspective of a scammer. Since spam filters are continuously improving, it is not easy to write spam mail that still passes those advanced filters. We will therefore analyze the predicted spam probability for various emails and try to find rules how spam filters can be fooled, so spam will still pass spam filters. The goal of this report is to describe and compare predictive modeling approaches for spam filtering based on a dataset where information about the content of emails and their classification as \"spam\" or \"non-spam\" has been collected. By applying interpretable machine learning methods, the inner workings of spam filters will be analyzed to bring more transparency to so-called black-box algorithms. Models and visualizations are implemented in R and Python using the packages \"mlr3\", \"ggplot\", \"iml\", \"DALEX\", \"scikitlearn\" and \"matplotlib\".","title":"Introduction"},{"location":"intro/#introduction","text":"Emails are an essential part of everyday life. They are used for communication between people both in private life and for business purposes. Generally, emails can be categorized as spam or non-spam, where non-spam emails contain useful or desired information for the recipient. Spam emails, on the other hand, are unwanted by the recipient and can be sent with the purpose of advertising, fraud, etc. According to the Trustwave Global Security Report 2020 , 45% of all emails sent in 2018 were spam. Nowadays, email service providers use spam filters based on various machine learning methods to distinguish between spam and non-spam emails. In the following report, we will take the rather unusual perspective of a scammer. Since spam filters are continuously improving, it is not easy to write spam mail that still passes those advanced filters. We will therefore analyze the predicted spam probability for various emails and try to find rules how spam filters can be fooled, so spam will still pass spam filters. The goal of this report is to describe and compare predictive modeling approaches for spam filtering based on a dataset where information about the content of emails and their classification as \"spam\" or \"non-spam\" has been collected. By applying interpretable machine learning methods, the inner workings of spam filters will be analyzed to bring more transparency to so-called black-box algorithms. Models and visualizations are implemented in R and Python using the packages \"mlr3\", \"ggplot\", \"iml\", \"DALEX\", \"scikitlearn\" and \"matplotlib\".","title":"Introduction"},{"location":"model/","text":"Modelling \u00b6 Since it is unknown what kind of spam filter the recipient uses, various machine learning algorithms are compared in terms of their ability to distinguish between spam and non-spam emails. To be able to assess the quality of each model a 5-fold cross-validation procedure is performed. Finally the interpretability of promising algorithms will be analyzed in Interpretable Machine Learning section. Evaluation Metric \u00b6 Probably the most straight forward metric to evaluate binary classification tasks is accuracy. Accuracy measures the fraction of correctly classified observations for a specific threshold value (usually 0.5). However, the AUC (area under the ROC curve) measures how true positive rate (recall) and false positive rate trade off and therefore evaluates the classifier for all possible threshold values. Compared to accuracy, AUC is a broader measure that tests the quality of the internal value, which the classifier generates and then compares it to a threshold. Since AUC does not test the quality of a particular choice of threshold, it will be used for the rest of the analysis. In our case, the AUC is equal to the probability that our model ranks a randomly chosen spam mail higher than a randomly chosen non-spam mail. Interpretable Algorithms \u00b6 Featureless Model \u00b6 As we are dealing with a binary classification task, a featureless model will be our very low baseline. As the name suggests, the featureless model takes no features/variables into account and always predicts the most frequent class for all observations. In this dataset, 60% of all emails are non-spam and therefore non-spam is predicted for all observations. When using the featureless model, it becomes clear why AUC suits this analysis better than accuracy. If always the most frequent class is predicted, the accuracy in our case is 0.6. The AUC however stays at its lowest possible value of 0.5 since the model cannot distinguish between spam and non-spam mails. Logistic Regression \u00b6 One algorithm which is inherently interpretable is logistic regression. Here we get a coefficient for each feature and could make statements like \u201cif feature x has a one unit increase the estimated spam probability changes by y, given all other features stay the same (\u2250 ceteris paribus)\u201d. Realistically, with nearly 60 features and the ceteris paribus assumption the interpretability of a regression model is more than questionable. Naive Bayes \u00b6 Another interpretable model is Naive Bayes. In this algorithm, the spam probability is calculated for each observation based on its feature values. With the naive assumption that all features are independent, it is then possible to interpret the conditional probability for each feature and therefore assess each feature's impact on the prediction. Decision Trees \u00b6 A single decision tree (cart) allows for very specific statements like \u201cIf feature A has value x and feature B has value y class Z will be predicted\u201d. Classification trees have the big advantage of being particularly easy to understand and therefore to interpret. However, such decision rules are quite unstable, oftentimes inaccurate and frequently lead to overfitting. Bagging unstable predictors is suggested (like decision trees) to improve accuracy. Bagging is short for \"Bootstrap Aggregating\" and is a procedure to average the prediction of multiple predictors to reduce the variance compared to a single predictor. This is the basis for random forests, which will be introduced in detail in the next section. Non-interpretable Algorithms \u00b6 Random Forest \u00b6 The first not easily interpretable model which is considered is random forest. It is an algorithm where multiple decision trees are combined, or aggregated, to improve the single tree\u2019s instability and tendency to overfit. Multiple decision trees are generated, where each tree can only use a subset of features (obtained by bootstrap) for splitting. However, the straight forward interpretability is lost compared to a single decision tree. For the random forest the parameter mtry in R, which is the number of variables to possibly split at in each node, is tuned prior to the benchmark in a 5-fold cross validation. The AUC optimal value is mtry = 7 . By using the data twice, once for hyperparameter tuning and once for the benchmark, we might underestimate the generalization error. However since hyperparameter tuning is computationally expensive, and we are not interested in the precise generalization error of each model but rather the general tendency of each model's performance, we refrained from nested resampling. Additionally, as it has shown in this article , random forests are an efficient way of classifying spam. We will therefore analyze this model class in more detail concerning its interpretability. Support Vector Machines \u00b6 Support vector machines (SVM) try to find a separating hyperplane between the two classes and are not interpretable as well. We refrained from using nested resampling for the same reasons as with random forest. We are not interested in the best possible model, but rather a realistic model, which might be applied in real life by email providers. Thus, in this analysis, a radial kernel will be used and the hyperparameters cost and gamma will be tuned in a 5-fold CV prior to the benchmark. The cost parameter controls how margin violations are penalized. High costs lead to expensive margin violations and therefore to a very narrow margin which in turn might promote overfitting. The parameter gamma from the kernel function k controls the shape of the decision boundary. k(u,v) = exp(-\\gamma(||u-v||^2)) A high \\gamma leads to a wiggly boundary which might also lead to overfitting. As you can see in the image below, both parameters have similar influences on the decision boundary and need to be balanced. As a wide range of combinations of gamma and cost might be suitable, we first tuned on a grid where gamma might take values in [10^{-8} , 10^2] and cost in [10^{-2}, 10^8] . A combination close to gamma = 0.01 and cost = 10 was the most promising and therefore a second 5-fold CV was performed for gamma values in [0.001, 0.05] and cost values in [5, 50]. The results of the tuning can be found below. The highest AUC of 0.978 could be achieved with gamma = 0.012 and cost = 13.33. As it has shown in this article , Support Vector Machines (SVM) are a powerful, state-of-the-art method in machine learning to distinguish spam from non-spam. Thus, we will analyze how SVMs obtain their predictions in more detail in Interpretable Machine Learning section. Performance of different cost and gamma combinations for SVM xGradient Boosting \u00b6 The last non-interpretable method is xGradient Boosting (xgboost) which uses gradient boosting to build the model. Here the parameters eta , min_child_weight , subsample , colsample_bytree , colsample_bylevel and nrounds will be tuned in a 5-fold CV with grid search. AUC optimal values can be found in Table below. Parameter lower bound upper bound AUC optimal value eta 0.2 0.4 0.33 subsample 0.7 0.8 0.77 colsample_bytree 0.9 1 0.93 colsample_bylevel 0.5 0.7 0.5 min_child_weight 1 20 1 nrounds 1 200 67 Benchmarking \u00b6 The distribution of 5-fold cross-validated AUC values for each model can be found in figure below. There you can also see that models which are inherently not interpretable, like SVM, random forest and xgboost outperformed explainable models like decision trees and logistic regression. It is very likely that email providers use such non-interpretable methods for their spam filters, especially since they are mostly interested in an accurate classification with as little false positives, i.e., emails that are wrongly classified as spam, as possible. Performance of prediction algorithms As previous research has shown, random forest and SVM are very well suitable algorithms for spam classification. Since xgboost has a slightly higher false positive rate than random forest as shown in the table below, we will only focus on SVM and random forest in our further analysis of the interpretability of these algorithms. In doing so, a kernel based (SVM) and a tree based (random forest) algorithm will be compared. learner AUC FPR featureless 0.500 0 svm 0.978 0.044 ranger 0.987 0.031 xgboost 0.988 0.032 If we now consider the perspective of scammers who want to make sure their emails pass arbitrary spam filters, it is necessary to make the predictions explainable. Personalization \u00b6 As described in Dataset section, the dataset contains variables on personal information of the recipient, like his name, George, his employer, Hewlett-Packard (HP) and the telephone area code of Palo Alto (650) where the office he works at is located. We are interested in the impact of personal knowledge on the classification. More precisely, we want to assess in what way the personalized features impact the prediction. A separate hyperparameter tuning procedure was performed for the non-personalized dataset, and the same parameters were found to be AUC-optimal. This is the reason the same hyperparameters are used for modeling spam probability with non-personalized features as for the personalized dataset. In the figure below, it can be seen that both SVM and Random Forest gain AUC when the personalized features are included. This is plausible, as knowing the name or workplace of the recipient is a strong indication that this email is from a known sender and not anonymous spam. Performance of SVM and random forest with personalized (all features) and non personalized features (removing \"george\", \"hp\", \"num650\")","title":"Modeling"},{"location":"model/#modelling","text":"Since it is unknown what kind of spam filter the recipient uses, various machine learning algorithms are compared in terms of their ability to distinguish between spam and non-spam emails. To be able to assess the quality of each model a 5-fold cross-validation procedure is performed. Finally the interpretability of promising algorithms will be analyzed in Interpretable Machine Learning section.","title":"Modelling"},{"location":"model/#evaluation-metric","text":"Probably the most straight forward metric to evaluate binary classification tasks is accuracy. Accuracy measures the fraction of correctly classified observations for a specific threshold value (usually 0.5). However, the AUC (area under the ROC curve) measures how true positive rate (recall) and false positive rate trade off and therefore evaluates the classifier for all possible threshold values. Compared to accuracy, AUC is a broader measure that tests the quality of the internal value, which the classifier generates and then compares it to a threshold. Since AUC does not test the quality of a particular choice of threshold, it will be used for the rest of the analysis. In our case, the AUC is equal to the probability that our model ranks a randomly chosen spam mail higher than a randomly chosen non-spam mail.","title":"Evaluation Metric"},{"location":"model/#interpretable-algorithms","text":"","title":"Interpretable Algorithms"},{"location":"model/#featureless-model","text":"As we are dealing with a binary classification task, a featureless model will be our very low baseline. As the name suggests, the featureless model takes no features/variables into account and always predicts the most frequent class for all observations. In this dataset, 60% of all emails are non-spam and therefore non-spam is predicted for all observations. When using the featureless model, it becomes clear why AUC suits this analysis better than accuracy. If always the most frequent class is predicted, the accuracy in our case is 0.6. The AUC however stays at its lowest possible value of 0.5 since the model cannot distinguish between spam and non-spam mails.","title":"Featureless Model"},{"location":"model/#logistic-regression","text":"One algorithm which is inherently interpretable is logistic regression. Here we get a coefficient for each feature and could make statements like \u201cif feature x has a one unit increase the estimated spam probability changes by y, given all other features stay the same (\u2250 ceteris paribus)\u201d. Realistically, with nearly 60 features and the ceteris paribus assumption the interpretability of a regression model is more than questionable.","title":"Logistic Regression"},{"location":"model/#naive-bayes","text":"Another interpretable model is Naive Bayes. In this algorithm, the spam probability is calculated for each observation based on its feature values. With the naive assumption that all features are independent, it is then possible to interpret the conditional probability for each feature and therefore assess each feature's impact on the prediction.","title":"Naive Bayes"},{"location":"model/#decision-trees","text":"A single decision tree (cart) allows for very specific statements like \u201cIf feature A has value x and feature B has value y class Z will be predicted\u201d. Classification trees have the big advantage of being particularly easy to understand and therefore to interpret. However, such decision rules are quite unstable, oftentimes inaccurate and frequently lead to overfitting. Bagging unstable predictors is suggested (like decision trees) to improve accuracy. Bagging is short for \"Bootstrap Aggregating\" and is a procedure to average the prediction of multiple predictors to reduce the variance compared to a single predictor. This is the basis for random forests, which will be introduced in detail in the next section.","title":"Decision Trees"},{"location":"model/#non-interpretable-algorithms","text":"","title":"Non-interpretable Algorithms"},{"location":"model/#random-forest","text":"The first not easily interpretable model which is considered is random forest. It is an algorithm where multiple decision trees are combined, or aggregated, to improve the single tree\u2019s instability and tendency to overfit. Multiple decision trees are generated, where each tree can only use a subset of features (obtained by bootstrap) for splitting. However, the straight forward interpretability is lost compared to a single decision tree. For the random forest the parameter mtry in R, which is the number of variables to possibly split at in each node, is tuned prior to the benchmark in a 5-fold cross validation. The AUC optimal value is mtry = 7 . By using the data twice, once for hyperparameter tuning and once for the benchmark, we might underestimate the generalization error. However since hyperparameter tuning is computationally expensive, and we are not interested in the precise generalization error of each model but rather the general tendency of each model's performance, we refrained from nested resampling. Additionally, as it has shown in this article , random forests are an efficient way of classifying spam. We will therefore analyze this model class in more detail concerning its interpretability.","title":"Random Forest"},{"location":"model/#support-vector-machines","text":"Support vector machines (SVM) try to find a separating hyperplane between the two classes and are not interpretable as well. We refrained from using nested resampling for the same reasons as with random forest. We are not interested in the best possible model, but rather a realistic model, which might be applied in real life by email providers. Thus, in this analysis, a radial kernel will be used and the hyperparameters cost and gamma will be tuned in a 5-fold CV prior to the benchmark. The cost parameter controls how margin violations are penalized. High costs lead to expensive margin violations and therefore to a very narrow margin which in turn might promote overfitting. The parameter gamma from the kernel function k controls the shape of the decision boundary. k(u,v) = exp(-\\gamma(||u-v||^2)) A high \\gamma leads to a wiggly boundary which might also lead to overfitting. As you can see in the image below, both parameters have similar influences on the decision boundary and need to be balanced. As a wide range of combinations of gamma and cost might be suitable, we first tuned on a grid where gamma might take values in [10^{-8} , 10^2] and cost in [10^{-2}, 10^8] . A combination close to gamma = 0.01 and cost = 10 was the most promising and therefore a second 5-fold CV was performed for gamma values in [0.001, 0.05] and cost values in [5, 50]. The results of the tuning can be found below. The highest AUC of 0.978 could be achieved with gamma = 0.012 and cost = 13.33. As it has shown in this article , Support Vector Machines (SVM) are a powerful, state-of-the-art method in machine learning to distinguish spam from non-spam. Thus, we will analyze how SVMs obtain their predictions in more detail in Interpretable Machine Learning section. Performance of different cost and gamma combinations for SVM","title":"Support Vector Machines"},{"location":"model/#xgradient-boosting","text":"The last non-interpretable method is xGradient Boosting (xgboost) which uses gradient boosting to build the model. Here the parameters eta , min_child_weight , subsample , colsample_bytree , colsample_bylevel and nrounds will be tuned in a 5-fold CV with grid search. AUC optimal values can be found in Table below. Parameter lower bound upper bound AUC optimal value eta 0.2 0.4 0.33 subsample 0.7 0.8 0.77 colsample_bytree 0.9 1 0.93 colsample_bylevel 0.5 0.7 0.5 min_child_weight 1 20 1 nrounds 1 200 67","title":"xGradient Boosting"},{"location":"model/#benchmarking","text":"The distribution of 5-fold cross-validated AUC values for each model can be found in figure below. There you can also see that models which are inherently not interpretable, like SVM, random forest and xgboost outperformed explainable models like decision trees and logistic regression. It is very likely that email providers use such non-interpretable methods for their spam filters, especially since they are mostly interested in an accurate classification with as little false positives, i.e., emails that are wrongly classified as spam, as possible. Performance of prediction algorithms As previous research has shown, random forest and SVM are very well suitable algorithms for spam classification. Since xgboost has a slightly higher false positive rate than random forest as shown in the table below, we will only focus on SVM and random forest in our further analysis of the interpretability of these algorithms. In doing so, a kernel based (SVM) and a tree based (random forest) algorithm will be compared. learner AUC FPR featureless 0.500 0 svm 0.978 0.044 ranger 0.987 0.031 xgboost 0.988 0.032 If we now consider the perspective of scammers who want to make sure their emails pass arbitrary spam filters, it is necessary to make the predictions explainable.","title":"Benchmarking"},{"location":"model/#personalization","text":"As described in Dataset section, the dataset contains variables on personal information of the recipient, like his name, George, his employer, Hewlett-Packard (HP) and the telephone area code of Palo Alto (650) where the office he works at is located. We are interested in the impact of personal knowledge on the classification. More precisely, we want to assess in what way the personalized features impact the prediction. A separate hyperparameter tuning procedure was performed for the non-personalized dataset, and the same parameters were found to be AUC-optimal. This is the reason the same hyperparameters are used for modeling spam probability with non-personalized features as for the personalized dataset. In the figure below, it can be seen that both SVM and Random Forest gain AUC when the personalized features are included. This is plausible, as knowing the name or workplace of the recipient is a strong indication that this email is from a known sender and not anonymous spam. Performance of SVM and random forest with personalized (all features) and non personalized features (removing \"george\", \"hp\", \"num650\")","title":"Personalization"}]}